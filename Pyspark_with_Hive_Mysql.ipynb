{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from os.path import abspath\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark-3.1.2-bin-hadoop3.2'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_201'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark-3.1.2-bin-hadoop3.2'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j                          \n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Examples\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TWO DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a hive database and write the two dataframes as hive tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView(\"empDF\")\n",
    "deptDF.createOrReplaceTempView(\"deptDF\")\n",
    "spark.sql(\"create table employee.emp_tbl as select * from empDF\")\n",
    "spark.sql(\"create table employee.dept_tbl as select * from deptDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN OPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF_deptDF_join = empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF_deptDF_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIVOT OPERATION / TRANSPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = empDF_deptDF_join.groupBy(\"dept_id\",\"dept_name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.createOrReplaceTempView(\"df_pivot\")\n",
    "df_trans = spark.sql(\"SELECT dept_id, STACK(1, 'dept_name', dept_name) AS (name,col) FROM df_pivot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|dept_id|     name|      col|\n",
      "+-------+---------+---------+\n",
      "|     10|dept_name|  Finance|\n",
      "|     20|dept_name|Marketing|\n",
      "|     40|dept_name|       IT|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+----+\n",
      "|dept_id|     COL1|COL2|COL3|\n",
      "+-------+---------+----+----+\n",
      "|     10|  Finance|null|null|\n",
      "|     20|Marketing|null|null|\n",
      "|     40|       IT|null|null|\n",
      "+-------+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trans.createOrReplaceTempView(\"df_trans\")\n",
    "df_trans = spark.sql(\"\"\"\n",
    "with Cte AS(\n",
    "    select *,\n",
    "        ROW_NUMBER() OVER(Partition by dept_id order by dept_id) as Rn\n",
    "        FROM df_trans)\n",
    "        select Cte.dept_id,\n",
    "        MAX(CASE WHEN Cte.Rn = 1 THEN Cte.col END) as COL1,\n",
    "        MAX(CASE WHEN Cte.Rn = 2 THEN Cte.col END) as COL2,\n",
    "        MAX(CASE WHEN Cte.Rn = 3 THEN Cte.col END) as COL3\n",
    "    FROM Cte\n",
    "    GROUP BY Cte.name, Cte.dept_id\n",
    "\"\"\")\n",
    "df_trans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNION OF DATFRAMES WITH DIFFERENT COLUMN NAMES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------+------+--------+------+---------------+-----------+\n",
      "|dept_id|dept_name|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|\n",
      "+-------+---------+-----------+------+------+--------+------+---------------+-----------+\n",
      "|   null|     null|         10|     1|     M|   Smith|  3000|             -1|       2018|\n",
      "|   null|     null|         20|     2|     M|    Rose|  4000|              1|       2010|\n",
      "|   null|     null|         10|     3|     M|Williams|  1000|              1|       2010|\n",
      "|   null|     null|         10|     4|     F|   Jones|  2000|              2|       2005|\n",
      "|   null|     null|         40|     5|      |   Brown|    -1|              2|       2010|\n",
      "|   null|     null|         50|     6|      |   Brown|    -1|              2|       2010|\n",
      "|     10|  Finance|       null|  null|  null|    null|  null|           null|       null|\n",
      "|     20|Marketing|       null|  null|  null|    null|  null|           null|       null|\n",
      "|     30|    Sales|       null|  null|  null|    null|  null|           null|       null|\n",
      "|     40|       IT|       null|  null|  null|    null|  null|           null|       null|\n",
      "+-------+---------+-----------+------+------+--------+------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql import Row\n",
    "\n",
    "def customUnion(df1,df2):\n",
    "    cols1 = [x.lower() for x in df1.columns]\n",
    "    cols2 = [x.lower() for x in df2.columns]\n",
    "    total_cols = sorted(cols1+list(set(cols2) - set(cols1)))\n",
    "    def expr(mycols, allcols):\n",
    "        def processCols(colname):\n",
    "            if colname in mycols:\n",
    "                return colname\n",
    "            else:\n",
    "                return lit(None).alias(colname)\n",
    "        cols = map(processCols, allcols)\n",
    "        return list(cols)\n",
    "    appended = df1.select(expr(cols1,total_cols)).union(df2.select(expr(cols2,total_cols)))\n",
    "    return appended\n",
    "df_union = customUnion(empDF,deptDF)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVING DUPLICATE COLUMN NAMES AFTER JOIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-----------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|emp_dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-----------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|         10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|         10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|         10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|         20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|         40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept1 = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns1 = [\"dept_name\",\"emp_dept_id\"]\n",
    "df3 = spark.createDataFrame(data=dept1, schema = deptColumns1)\n",
    "df3 = empDF.join(df3,empDF.emp_dept_id == df3.emp_dept_id, 'inner')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_new = []\n",
    "seen = set()\n",
    "for c in df3.columns:\n",
    "    c = c.lower()\n",
    "    cols_new.append('{}_dup'.format(c) if c in seen else c)\n",
    "    seen.add(c)\n",
    "df3 = df3.toDF(*cols_new).select(*[c for c in cols_new if not c.endswith('_dup')])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYSPARK WITH MYSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcHostname = \"use your hostname\"\n",
    "jdbcDatabase = \"use your database name\"\n",
    "jdbcPort = 3306\n",
    "jdbcUrl = \"jdbc:mysql://{0}:{1}/{2}?user={3}&password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from tablename) emp_alias\"\n",
    "df1 = spark.read.jdbc(url=jdbcUrl, table=pushdown_query)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
