{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.1.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark-3.1.2-bin-hadoop3.2'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_201'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark-3.1.2-bin-hadoop3.2'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Basic Examples\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('F:/main course/Artificial_Neural_Networks/AppleStore.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- size_bytes: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- rating_count_tot: integer (nullable = true)\n",
      " |-- rating_count_ver: integer (nullable = true)\n",
      " |-- user_rating_ver: double (nullable = true)\n",
      " |-- cont_rating: integer (nullable = true)\n",
      " |-- prime_genre: string (nullable = true)\n",
      " |-- sup_devices.num: integer (nullable = true)\n",
      " |-- ipadSc_urls.num: integer (nullable = true)\n",
      " |-- lang.num: integer (nullable = true)\n",
      " |-- vpp_lic: integer (nullable = true)\n",
      " |-- user_rating: double (nullable = true)\n",
      "\n",
      "+----------+-----+----------------+----------------+---------------+-----------+-----------------+---------------+---------------+--------+-------+-----------+\n",
      "|size_bytes|price|rating_count_tot|rating_count_ver|user_rating_ver|cont_rating|prime_genre      |sup_devices.num|ipadSc_urls.num|lang.num|vpp_lic|user_rating|\n",
      "+----------+-----+----------------+----------------+---------------+-----------+-----------------+---------------+---------------+--------+-------+-----------+\n",
      "|100788224 |3.99 |21292           |26              |4.5            |4          |Games            |38             |5              |10      |1      |4.0        |\n",
      "|158578688 |0.0  |161065          |26              |3.5            |4          |Productivity     |37             |5              |23      |1      |4.0        |\n",
      "|100524032 |0.0  |188583          |2822            |4.5            |4          |Weather          |37             |5              |3       |1      |3.5        |\n",
      "|128512000 |0.0  |262241          |649             |4.5            |12         |Shopping         |37             |5              |9       |1      |4.0        |\n",
      "|92774400  |0.0  |985920          |5320            |5.0            |4          |Reference        |37             |5              |45      |1      |4.5        |\n",
      "|10485713  |0.99 |8253            |5516            |4.0            |4          |Games            |47             |5              |1       |1      |4.0        |\n",
      "|227795968 |0.0  |119487          |879             |4.5            |4          |Finance          |37             |0              |19      |1      |4.0        |\n",
      "|130242560 |0.0  |1126879         |3594            |4.5            |12         |Music            |37             |4              |1       |1      |4.0        |\n",
      "|49250304  |9.99 |1117            |4               |5.0            |4          |Utilities        |37             |5              |1       |1      |4.5        |\n",
      "|70023168  |3.99 |7885            |40              |4.0            |4          |Games            |38             |0              |10      |1      |4.0        |\n",
      "|49618944  |4.99 |76720           |4017            |4.5            |4          |Games            |38             |4              |11      |1      |4.5        |\n",
      "|227547136 |7.99 |105776          |166             |2.5            |4          |Games            |37             |0              |6       |1      |3.5        |\n",
      "|179979264 |0.0  |479440          |203             |4.0            |17         |Utilities        |37             |4              |33      |1      |3.5        |\n",
      "|160925696 |0.0  |119773          |2336            |4.5            |4          |Finance          |37             |0              |2       |1      |3.5        |\n",
      "|55153664  |4.99 |6340            |668             |4.5            |4          |Games            |38             |5              |2       |1      |4.5        |\n",
      "|207907840 |0.0  |56194           |87              |3.5            |4          |Travel           |37             |1              |26      |1      |4.0        |\n",
      "|389879808 |0.0  |2974676         |212             |3.5            |4          |Social Networking|37             |1              |29      |1      |3.5        |\n",
      "|167407616 |0.0  |223885          |3726            |4.5            |12         |Travel           |37             |5              |18      |1      |4.0        |\n",
      "|147093504 |0.0  |402925          |136             |4.5            |12         |Music            |37             |3              |16      |1      |4.0        |\n",
      "|10735026  |2.99 |31456           |4178            |3.5            |4          |Games            |47             |0              |1       |1      |4.0        |\n",
      "+----------+-----+----------------+----------------+---------------+-----------+-----------------+---------------+---------------+--------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(['size_bytes','price','prime_genre','user_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast operation \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "df_transformed = df.withColumn(\"price_new\",col(\"price\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF to left pad the string with '0s' in a column\n",
    "from pyspark.sql.functions import udf\n",
    "def leftPad(string):\n",
    "    string = str(string).rjust(10,'0')\n",
    "    return string\n",
    "\n",
    "convertUDF = udf(lambda z: leftPad(z),StringType())\n",
    "\n",
    "# Creating a UDF to change the values to Upper Case\n",
    "convertUDF2 = udf(lambda z: str(z).upper(),StringType())\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\"price_new\",convertUDF(col(\"price_new\")).alias(\"price_new\"))\n",
    "df_transformed = df_transformed.withColumn(\"prime_genre\",convertUDF2(col(\"prime_genre\")).alias(\"prime_genre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substring and trim\n",
    "from pyspark.sql.functions import substring, trim\n",
    "df_transformed = df_transformed.withColumn(\"price_new\",substring('price_new',6,4))\n",
    "df_transformed = df_transformed.withColumn('prime_genre', trim('prime_genre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+-----------+---------+\n",
      "|size_bytes|price|      prime_genre|user_rating|price_new|\n",
      "+----------+-----+-----------------+-----------+---------+\n",
      "| 100788224| 3.99|            GAMES|        4.0|     03.9|\n",
      "| 158578688|  0.0|     PRODUCTIVITY|        4.0|     000.|\n",
      "| 100524032|  0.0|          WEATHER|        3.5|     000.|\n",
      "| 128512000|  0.0|         SHOPPING|        4.0|     000.|\n",
      "|  92774400|  0.0|        REFERENCE|        4.5|     000.|\n",
      "|  10485713| 0.99|            GAMES|        4.0|     00.9|\n",
      "| 227795968|  0.0|          FINANCE|        4.0|     000.|\n",
      "| 130242560|  0.0|            MUSIC|        4.0|     000.|\n",
      "|  49250304| 9.99|        UTILITIES|        4.5|     09.9|\n",
      "|  70023168| 3.99|            GAMES|        4.0|     03.9|\n",
      "|  49618944| 4.99|            GAMES|        4.5|     04.9|\n",
      "| 227547136| 7.99|            GAMES|        3.5|     07.9|\n",
      "| 179979264|  0.0|        UTILITIES|        3.5|     000.|\n",
      "| 160925696|  0.0|          FINANCE|        3.5|     000.|\n",
      "|  55153664| 4.99|            GAMES|        4.5|     04.9|\n",
      "| 207907840|  0.0|           TRAVEL|        4.0|     000.|\n",
      "| 389879808|  0.0|SOCIAL NETWORKING|        3.5|     000.|\n",
      "| 167407616|  0.0|           TRAVEL|        4.0|     000.|\n",
      "| 147093504|  0.0|            MUSIC|        4.0|     000.|\n",
      "|  10735026| 2.99|            GAMES|        4.0|     02.9|\n",
      "+----------+-----+-----------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|prime_genre_count|\n",
      "+-----------------+\n",
      "|               10|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Distinct\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df_distinct_count = df_transformed.select(countDistinct(\"prime_genre\").alias(\"prime_genre_count\"))\n",
    "df_distinct_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+--------------------+------------------+\n",
      "|      prime_genre|frequency|Percent|cumulative_frequency|cumulative_Percent|\n",
      "+-----------------+---------+-------+--------------------+------------------+\n",
      "|            GAMES|        7|   35.0|                   7|              35.0|\n",
      "|        UTILITIES|        2|   10.0|                  15|              75.0|\n",
      "|            MUSIC|        2|   10.0|                  15|              75.0|\n",
      "|           TRAVEL|        2|   10.0|                  15|              75.0|\n",
      "|          FINANCE|        2|   10.0|                  15|              75.0|\n",
      "|          WEATHER|        1|    5.0|                  20|             100.0|\n",
      "|SOCIAL NETWORKING|        1|    5.0|                  20|             100.0|\n",
      "|        REFERENCE|        1|    5.0|                  20|             100.0|\n",
      "|         SHOPPING|        1|    5.0|                  20|             100.0|\n",
      "|     PRODUCTIVITY|        1|    5.0|                  20|             100.0|\n",
      "+-----------------+---------+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frequency, cummulative frequency and cumulative percentage\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "frequencies = df_transformed.groupBy('prime_genre').agg(\n",
    "    count('prime_genre').alias('frequency')\n",
    ").selectExpr(\n",
    "    '*','100*Frequency / sum(Frequency) over() Percent'\n",
    ").selectExpr('*',\n",
    "            'sum(Frequency) over(order by Frequency desc) cumulative_frequency',\n",
    "            'sum(Percent) over(order by Frequency desc) cumulative_Percent'\n",
    ")\n",
    "frequencies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|Null_value|Not_Nul_values|\n",
      "+----------+--------------+\n",
      "|         0|            20|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe showing count of null and not null values from a given column\n",
    "\n",
    "from pyspark.sql.functions import isnan\n",
    "null_values = df_transformed.filter(df_transformed.price.contains('None') | \\\n",
    "                                   df_transformed.price.contains('NULL') | \\\n",
    "                                   (col(\"price\") == '') | \\\n",
    "                                   isnan(df_transformed.price) | \\\n",
    "                                   df_transformed.price.isNull()\n",
    "                                   ).count()\n",
    "\n",
    "Not_null_values = df_transformed.count() - null_values\n",
    "data = [(null_values, Not_null_values)]\n",
    "df_new = spark.createDataFrame(data,[\"Null_value\",\"Not_Nul_values\"])\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----------+---------+\n",
      "|size_bytes|price|prime_genre|user_rating|price_new|\n",
      "+----------+-----+-----------+-----------+---------+\n",
      "| 179979264|  0.0|  UTILITIES|        3.5|     000.|\n",
      "|  55153664| 4.99|      GAMES|        4.5|     04.9|\n",
      "| 100788224| 3.99|      GAMES|        4.0|     03.9|\n",
      "|  10485713| 0.99|      GAMES|        4.0|     00.9|\n",
      "|  70023168| 3.99|      GAMES|        4.0|     03.9|\n",
      "|  10735026| 2.99|      GAMES|        4.0|     02.9|\n",
      "| 227547136| 7.99|      GAMES|        3.5|     07.9|\n",
      "| 147093504|  0.0|      MUSIC|        4.0|     000.|\n",
      "| 167407616|  0.0|     TRAVEL|        4.0|     000.|\n",
      "| 160925696|  0.0|    FINANCE|        3.5|     000.|\n",
      "+----------+-----+-----------+-----------+---------+\n",
      "\n",
      "+----------+-----+-----------------+-----------+---------+\n",
      "|size_bytes|price|      prime_genre|user_rating|price_new|\n",
      "+----------+-----+-----------------+-----------+---------+\n",
      "| 100524032|  0.0|          WEATHER|        3.5|     000.|\n",
      "|  49250304| 9.99|        UTILITIES|        4.5|     09.9|\n",
      "|  49618944| 4.99|            GAMES|        4.5|     04.9|\n",
      "| 389879808|  0.0|SOCIAL NETWORKING|        3.5|     000.|\n",
      "|  92774400|  0.0|        REFERENCE|        4.5|     000.|\n",
      "| 130242560|  0.0|            MUSIC|        4.0|     000.|\n",
      "| 128512000|  0.0|         SHOPPING|        4.0|     000.|\n",
      "| 207907840|  0.0|           TRAVEL|        4.0|     000.|\n",
      "| 158578688|  0.0|     PRODUCTIVITY|        4.0|     000.|\n",
      "| 227795968|  0.0|          FINANCE|        4.0|     000.|\n",
      "+----------+-----+-----------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create two dataframes , one will have all the unique values and the other will have all the values which are dropped from the main dataframe\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "w2 = Window.partitionBy('prime_genre').orderBy(col(\"user_rating\").desc())\n",
    "df_duplicate = df_transformed.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\")>1).drop(\"row\")\n",
    "df_unique = df_transformed.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\")==1).drop(\"row\")\n",
    "df_duplicate.show()\n",
    "df_unique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
